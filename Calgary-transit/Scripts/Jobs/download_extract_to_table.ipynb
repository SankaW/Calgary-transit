{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3496ab9-39b5-4481-b020-f6c4c9071cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download for the source and save in the landing volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96bd06f5-f5b9-41bd-826b-fdd940287159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://data.calgary.ca/download/npk7-z3bj/application%2Fx-zip-compressed\"\n",
    "\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "zip_path = f\"/Volumes/calgary_transit/landing/landing_vol/calgary_gtfs_{date_str}.zip\"\n",
    "\n",
    "# stream download (safer for larger files)\n",
    "with requests.get(url, stream=True, timeout=120) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"Saved:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f16fb1-2db2-4dea-b673-6d909f5d7e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unzip into Bronze volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcf54ad-971b-4d5b-8ea8-77df7e52ae70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "extract_dir = f\"/Volumes/calgary_transit/bronze/bronze_vol/calgary_gtfs/{date_str}/\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "    z.extractall(extract_dir)\n",
    "\n",
    "print(\"Extracted to:\", extract_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e7f6f8-95f3-4c8c-953c-cacbf4f15df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze volume to Bronze tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac2e3b8-b906-44fe-88dc-cb0141fff9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import os, glob\n",
    "from datetime import datetime\n",
    "\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")  # keep consistent with your run\n",
    "extract_dir = f\"/Volumes/calgary_transit/bronze/bronze_vol/calgary_gtfs/{date_str}/\"\n",
    "\n",
    "# Where tables will live\n",
    "catalog = \"calgary_transit\"\n",
    "schema = \"bronze\"   # <-- your bronze schema that holds Delta tables\n",
    "table_prefix = \"gtfs_\"  # results like calgary_transit.bronze.gtfs_stops\n",
    "\n",
    "txt_files = glob.glob(os.path.join(extract_dir, \"*.txt\"))\n",
    "\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in: {extract_dir}\")\n",
    "\n",
    "ingest_ts = F.current_timestamp()\n",
    "\n",
    "for fp in sorted(txt_files):\n",
    "    base = os.path.splitext(os.path.basename(fp))[0]   # stops, routes, trips, ...\n",
    "    table_name = f\"{catalog}.{schema}.{table_prefix}{base}\"\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .option(\"mode\", \"PERMISSIVE\")\n",
    "            .csv(fp)\n",
    "            .withColumn(\"_ingest_date\", F.lit(date_str))\n",
    "            .withColumn(\"_ingest_ts\", ingest_ts)\n",
    "            .withColumn(\"_source_file\", F.lit(fp))\n",
    "    )\n",
    "\n",
    "    # Append (creates table on first run)\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"append\")\n",
    "       .option(\"mergeSchema\", \"true\")   # helpful if columns evolve slightly\n",
    "       .saveAsTable(table_name))\n",
    "\n",
    "    print(f\"Appended {df.count():,} rows -> {table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "download_extract_to_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
