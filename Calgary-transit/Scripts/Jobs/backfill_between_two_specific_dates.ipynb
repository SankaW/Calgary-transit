{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4de41e73-82a6-41a8-9cbc-40ac4771d5a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# STEP 1: Job Parameters\n",
    "# -----------------------------------------\n",
    "\n",
    "dbutils.widgets.text(\"start_date\", \"\")\n",
    "dbutils.widgets.text(\"end_date\", \"\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date_str = dbutils.widgets.get(\"start_date\")\n",
    "end_date_str = dbutils.widgets.get(\"end_date\")\n",
    "\n",
    "if not start_date_str:\n",
    "    start_date = datetime.now().date()\n",
    "else:\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "if not end_date_str:\n",
    "    end_date = start_date\n",
    "else:\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "print(f\"Processing from {start_date} to {end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "991e87c9-313d-4d3e-80cc-b3fe6aa4f4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# STEP 2: Bronze Ingestion Logic\n",
    "# -----------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import os, glob\n",
    "\n",
    "catalog = \"calgary_transit\"\n",
    "schema = \"bronze\"\n",
    "table_prefix = \"gtfs_\"\n",
    "\n",
    "ingest_ts = F.current_timestamp()\n",
    "\n",
    "d = start_date\n",
    "while d <= end_date:\n",
    "    date_str = d.strftime(\"%Y-%m-%d\")\n",
    "    extract_dir = f\"/Volumes/calgary_transit/bronze/bronze_vol/calgary_gtfs/{date_str}/\"\n",
    "\n",
    "    txt_files = glob.glob(os.path.join(extract_dir, \"*.txt\"))\n",
    "\n",
    "    if not txt_files:\n",
    "        print(f\"⚠️ No files found for {date_str}\")\n",
    "        d += timedelta(days=1)\n",
    "        continue\n",
    "\n",
    "    for fp in sorted(txt_files):\n",
    "        base = os.path.splitext(os.path.basename(fp))[0]\n",
    "        table_name = f\"{catalog}.{schema}.{table_prefix}{base}\"\n",
    "\n",
    "        df = (\n",
    "            spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .option(\"mode\", \"PERMISSIVE\")\n",
    "                .csv(fp)\n",
    "                .withColumn(\"_ingest_date\", F.lit(date_str))\n",
    "                .withColumn(\"_ingest_ts\", ingest_ts)\n",
    "                .withColumn(\"_source_file\", F.lit(fp))\n",
    "        )\n",
    "\n",
    "        (df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "\n",
    "        print(f\"✅ {date_str}: appended -> {table_name}\")\n",
    "\n",
    "    d += timedelta(days=1)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "backfill_between_two_specific_dates",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
